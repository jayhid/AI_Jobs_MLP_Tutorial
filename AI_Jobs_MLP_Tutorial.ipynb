{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1b6a8279",
      "metadata": {
        "id": "1b6a8279"
      },
      "source": [
        "# Predicting Job Automation Risk with Multilayer Perceptrons (MLPs)\n",
        "\n",
        "In this tutorial-style notebook, we use a Multilayer Perceptron (MLP) to predict the **job automation risk category** by 2030 using the `AI_Impact_on_Jobs_2030` dataset.\n",
        "\n",
        "We will:\n",
        "- Explore the dataset with visualisations.\n",
        "- Build a preprocessing pipeline for numeric + categorical features.\n",
        "- Train a baseline MLP classifier for `Risk_Category`.\n",
        "- Systematically vary **depth** and **width** of the MLP and measure performance.\n",
        "- Train a tuned MLP and evaluate it.\n",
        "- Use multiple graphs: heatmaps, confusion matrices, learning curve, PCA plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4578f15e",
      "metadata": {
        "id": "4578f15e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    StratifiedKFold,\n",
        "    cross_val_score,\n",
        "    learning_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (8, 5)\n",
        "\n",
        "print('Libraries imported.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e498218",
      "metadata": {
        "id": "8e498218"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/AI_Impact_on_Jobs_2030.csv'  # change if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print('Shape:', df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0QKNTcWDkXah"
      },
      "id": "0QKNTcWDkXah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c5085a",
      "metadata": {
        "id": "c0c5085a"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caca859b",
      "metadata": {
        "id": "caca859b"
      },
      "outputs": [],
      "source": [
        "risk_counts = df['Risk_Category'].value_counts().sort_index()\n",
        "print(risk_counts)\n",
        "\n",
        "plt.figure()\n",
        "sns.barplot(x=risk_counts.index, y=risk_counts.values)\n",
        "plt.title('Distribution of Risk Categories')\n",
        "plt.xlabel('Risk Category')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "248d9424",
      "metadata": {
        "id": "248d9424"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "sns.histplot(df['Automation_Probability_2030'], bins=20, kde=True)\n",
        "plt.title('Distribution of Automation Probability (2030)')\n",
        "plt.xlabel('Automation Probability 2030')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0325c245",
      "metadata": {
        "id": "0325c245"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "sns.boxplot(data=df, x='Risk_Category', y='Average_Salary')\n",
        "plt.title('Average Salary by Risk Category')\n",
        "plt.xlabel('Risk Category')\n",
        "plt.ylabel('Average Salary')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f895494a",
      "metadata": {
        "id": "f895494a"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "sns.scatterplot(\n",
        "    data=df,\n",
        "    x='AI_Exposure_Index',\n",
        "    y='Automation_Probability_2030',\n",
        "    hue='Risk_Category'\n",
        ")\n",
        "plt.title('AI Exposure vs Automation Probability')\n",
        "plt.xlabel('AI Exposure Index')\n",
        "plt.ylabel('Automation Probability 2030')\n",
        "plt.legend(title='Risk Category')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e4ca4bf",
      "metadata": {
        "id": "8e4ca4bf"
      },
      "outputs": [],
      "source": [
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "corr = df[numeric_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, annot=False, cmap='viridis')\n",
        "plt.title('Correlation Heatmap (Numeric Features)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930f0a32",
      "metadata": {
        "id": "930f0a32"
      },
      "outputs": [],
      "source": [
        "target_col = 'Risk_Category'\n",
        "\n",
        "feature_cols = [\n",
        "    'Average_Salary', 'Years_Experience',\n",
        "    'AI_Exposure_Index', 'Tech_Growth_Factor',\n",
        "    'Skill_1', 'Skill_2', 'Skill_3', 'Skill_4', 'Skill_5',\n",
        "    'Skill_6', 'Skill_7', 'Skill_8', 'Skill_9', 'Skill_10',\n",
        "    'Education_Level'\n",
        "]\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "y = df[target_col].copy()\n",
        "\n",
        "categorical_features = ['Education_Level']\n",
        "numeric_features = [col for col in feature_cols if col not in categorical_features]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print('Train shape:', X_train.shape)\n",
        "print('Test shape:', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e60dcd",
      "metadata": {
        "id": "a1e60dcd"
      },
      "outputs": [],
      "source": [
        "# Use sparse_output=False (or sparse=False on older sklearn) to return dense arrays\n",
        "try:\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "except TypeError:\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "baseline_mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 64),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=300,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "baseline_model = Pipeline(steps=[\n",
        "    ('preprocess', preprocess),\n",
        "    ('mlp', baseline_mlp)\n",
        "])\n",
        "\n",
        "baseline_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_baseline = baseline_model.predict(X_test)\n",
        "baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
        "print(f'Baseline MLP Test Accuracy: {baseline_acc:.4f}')\n",
        "print('\\nClassification Report (Baseline):')\n",
        "print(classification_report(y_test, y_pred_baseline))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "609815c7",
      "metadata": {
        "id": "609815c7"
      },
      "outputs": [],
      "source": [
        "labels_sorted = sorted(y.unique())\n",
        "cm = confusion_matrix(y_test, y_pred_baseline, labels=labels_sorted)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_sorted)\n",
        "\n",
        "plt.figure()\n",
        "disp.plot(values_format='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Baseline MLP')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bd4a0c7",
      "metadata": {
        "id": "1bd4a0c7"
      },
      "outputs": [],
      "source": [
        "depth_options = [1, 2, 3]\n",
        "width_options = [16, 64, 128]\n",
        "\n",
        "results = []\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for depth in depth_options:\n",
        "    for width in width_options:\n",
        "        layers = tuple([width] * depth)\n",
        "        mlp = MLPClassifier(\n",
        "            hidden_layer_sizes=layers,\n",
        "            activation='relu',\n",
        "            solver='adam',\n",
        "            learning_rate_init=0.001,\n",
        "            max_iter=300,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model = Pipeline(steps=[\n",
        "            ('preprocess', preprocess),\n",
        "            ('mlp', mlp)\n",
        "        ])\n",
        "\n",
        "        scores = cross_val_score(\n",
        "            model,\n",
        "            X_train,\n",
        "            y_train,\n",
        "            cv=cv,\n",
        "            scoring='accuracy'\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'depth': depth,\n",
        "            'width': width,\n",
        "            'mean_accuracy': scores.mean(),\n",
        "            'std_accuracy': scores.std()\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.sort_values(by='mean_accuracy', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36a5420",
      "metadata": {
        "id": "c36a5420"
      },
      "outputs": [],
      "source": [
        "pivot = results_df.pivot(index='depth', columns='width', values='mean_accuracy')\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(pivot, annot=True, fmt='.3f', cmap='viridis')\n",
        "plt.title('Cross-Validated Accuracy for Different MLP Architectures')\n",
        "plt.xlabel('Width (neurons per hidden layer)')\n",
        "plt.ylabel('Depth (number of hidden layers)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81e3b54a",
      "metadata": {
        "id": "81e3b54a"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "for width in width_options:\n",
        "    subset = results_df[results_df['width'] == width].sort_values('depth')\n",
        "    plt.plot(\n",
        "        subset['depth'],\n",
        "        subset['mean_accuracy'],\n",
        "        marker='o',\n",
        "        label=f'width={width}'\n",
        "    )\n",
        "\n",
        "plt.title('Effect of Depth and Width on Accuracy')\n",
        "plt.xlabel('Depth (hidden layers)')\n",
        "plt.ylabel('Mean CV Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee7778e",
      "metadata": {
        "id": "9ee7778e"
      },
      "outputs": [],
      "source": [
        "best_row = results_df.sort_values('mean_accuracy', ascending=False).iloc[0]\n",
        "best_depth = int(best_row['depth'])\n",
        "best_width = int(best_row['width'])\n",
        "best_layers = tuple([best_width] * best_depth)\n",
        "\n",
        "print('Best depth:', best_depth)\n",
        "print('Best width:', best_width)\n",
        "print('Best hidden_layer_sizes:', best_layers)\n",
        "\n",
        "best_mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=best_layers,\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=300,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "best_model = Pipeline(steps=[\n",
        "    ('preprocess', preprocess),\n",
        "    ('mlp', best_mlp)\n",
        "])\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "best_acc = accuracy_score(y_test, y_pred_best)\n",
        "print(f'Best MLP Test Accuracy: {best_acc:.4f}')\n",
        "print('\\nClassification Report (Best Model):')\n",
        "print(classification_report(y_test, y_pred_best))\n",
        "\n",
        "cm_best = confusion_matrix(y_test, y_pred_best, labels=labels_sorted)\n",
        "disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best, display_labels=labels_sorted)\n",
        "\n",
        "plt.figure()\n",
        "disp_best.plot(values_format='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Best MLP Model')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff6d2a1",
      "metadata": {
        "id": "6ff6d2a1"
      },
      "outputs": [],
      "source": [
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    best_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "train_std = train_scores.std(axis=1)\n",
        "val_mean = val_scores.mean(axis=1)\n",
        "val_std = val_scores.std(axis=1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_sizes, train_mean, marker='o', label='Training accuracy')\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
        "\n",
        "plt.plot(train_sizes, val_mean, marker='o', label='Validation accuracy')\n",
        "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2)\n",
        "\n",
        "plt.title('Learning Curve - Best MLP Model')\n",
        "plt.xlabel('Number of training samples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f350f88",
      "metadata": {
        "id": "8f350f88"
      },
      "outputs": [],
      "source": [
        "# PCA visualisation of preprocessed features\n",
        "X_processed = preprocess.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_processed)\n",
        "\n",
        "pca_df = pd.DataFrame({\n",
        "    'PC1': X_pca[:, 0],\n",
        "    'PC2': X_pca[:, 1],\n",
        "    'Risk_Category': y.values\n",
        "})\n",
        "\n",
        "plt.figure()\n",
        "sns.scatterplot(\n",
        "    data=pca_df,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    hue='Risk_Category',\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title('PCA Projection of Jobs Coloured by Risk Category')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend(title='Risk Category')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}